
# 必填
dataset: /home/zjc/data/flicker
dataset_kodak: /home/zjc/data/kodak/
dataset_clic: /home/zjc/MoECodec/clic2020pro/

# 训练输出
out_dir: ./checkpoints/spatial/

# 训练设置
epochs: 31
batch_size: 18
test_batch_size: 1
num_workers: 16
patch_size: 256
seed: 42
clip_max_norm: 1.0
save: true
train_mode: all

# 模型: TIC, TIC_MoE, TinyLIC, TinyLIC_MoE, TCM, TCM_MoE
# 各模型默认 N/M:
#   TIC / TIC_MoE:       N=128, M=192
#   TinyLIC / TinyLIC_MoE: N=128, M=320
#   TCM / TCM_MoE:       N=64,  M=320, Z=192
model: TCM_MoE
N: 64
M: 320
Z: 192

# RD loss, mse系数
lmbda: 1e-3
distortion: mse

# 优化器
learning_rate: 0.0001
aux_learning_rate: 0.001
cosine_eta_min: 0.000001

# scheduler
milestones: [5,15,20]
gamma: 0.5

# device
cuda: true
gpu_id: 0

checkpoint: "/home/zjc/MoECodec/LIC_TCM/chexkpoints/0.013.pth..tar"
TEST: False

#moe
enc_moe: True
dec_moe: True
h_moe: False
moe_config:
  # moe_type:
  #   "spatial" (默认, 像素级路由)
  #   "channel" (对通道路由)
  #   "patch"   (先 patchify 再路由, patch token = patch_size x patch_size 像素块)
  moe_type: spatial
  expert_type: lr_mlp
  lr_rank: 32
  # mix_batch_token: 仅 spatial SparseMoE 生效；True 时将 B 和 token 维合并后全局路由
  mix_batch_token: True
  # vis_region_size: energy-vs-compute 可视化时，多少像素聚合成1个token
  vis_region_size: 4
  # patch_size: patchify 的块大小 (仅 patch MoE 使用), 推荐 2 或 4
  patch_size: 4
  # num_groups: 通道组数 (仅 channel MoE 使用), 必须整除所有 MoE 层的通道数
  num_groups: 8
  num_experts: 4
  capacity: 1.0
  n_shared_experts: 0
  hid_ratio: 4
  # 邻域一致性正则系数: L_smooth = sum_i ||∇p_i||_1
  smooth_lambda: 0.001
