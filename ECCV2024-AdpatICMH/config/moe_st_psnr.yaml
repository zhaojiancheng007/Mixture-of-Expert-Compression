
# 必填
dataset: /home/zjc/data/flicker
dataset_kodak: /home/zjc/data/kodak/
dataset_clic: /home/zjc/data/CLICval2017/

# 训练输出
out_dir: ./checkpoints/
name: TinyLIC_MoE

# 训练设置
epochs: 20
batch_size: 18
test_batch_size: 1
num_workers: 16
patch_size: 256
seed: 42
clip_max_norm: 1.0
save: true

# 模型: TIC, TIC_MoE, TinyLIC, TinyLIC_MoE, TCM, TCM_MoE
# 各模型默认 N/M:
#   TIC / TIC_MoE:       N=128, M=192
#   TinyLIC / TinyLIC_MoE: N=128, M=320
#   TCM / TCM_MoE:       N=64,  M=320, Z=192
model: TinyLIC_MoE
N: 128
M: 320
# Z: 192     # 超先验瓶颈通道数 (仅 TCM/TCM_MoE 使用)

# RD loss, mse系数
lmbda: 5e-3
distortion: mse

# 优化器
learning_rate: 0.0001
aux_learning_rate: 0.001

# scheduler
milestones: [5,10,15]
gamma: 0.1

# device
cuda: true
gpu_id: 0

checkpoint: "/home/zjc/MoECodec/TinyLIC/pretrained/mse/checkpoint_q1.pth.tar"
TEST: False

#moe
enc_moe: True
dec_moe: True
h_moe: True
moe_config:
  # moe_type: "spatial" (默认, 对空间位置路由) 或 "channel" (对通道组路由)
  moe_type: channel
  # num_groups: 通道组数 (仅 channel MoE 使用), 必须整除所有 MoE 层的通道数
  #   TIC/TIC_MoE: 需整除 128, 192 → 可选 4, 8, 16, 32, 64
  #   TinyLIC/TinyLIC_MoE: 需整除 128, 192, 256, 320 → 可选 4, 8, 16, 32, 64
  #   TCM/TCM_MoE: 需整除 128, 320 → 可选 4, 8, 16, 32, 64
  num_groups: 8
  num_experts: 4
  capacity: 1.0
  n_shared_experts: 0
  hid_ratio: 4